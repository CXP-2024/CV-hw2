{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT Feature Detection and Matching Demo\n",
    "\n",
    "This notebook demonstrates how to use the custom SIFT implementation for feature detection, description, and matching between images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2  # Only for basic image operations\n",
    "# from sift_useful import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "def my_conv2d(kernel, img, conv_type=\"full\"):\n",
    "    \"\"\"\n",
    "    Implement 2D image convolution\n",
    "    :param kernel: float/int array, shape: (x, x)\n",
    "    :param img: float/int array, shape: (height, width)\n",
    "    :param conv_type: str, convolution padding choices, should in ['full', 'same', 'valid']\n",
    "    :return conv results, numpy array\n",
    "    \"\"\"\n",
    "    k_h, k_w = kernel.shape\n",
    "    i_h, i_w = img.shape\n",
    "\n",
    "    kernel = np.rot90(kernel, 2)\n",
    "\n",
    "    # Calculate output dimensions based on conv_type\n",
    "    half_k_h = k_h // 2\n",
    "    half_k_w = k_w // 2\n",
    "    if conv_type == \"full\":\n",
    "        pad_h = k_h - 1\n",
    "        pad_w = k_w - 1\n",
    "        out_h = i_h + pad_h\n",
    "        out_w = i_w + pad_w\n",
    "    elif conv_type == \"same\":\n",
    "        pad_h = k_h // 2\n",
    "        pad_w = k_w // 2\n",
    "        out_h = i_h\n",
    "        out_w = i_w\n",
    "    elif conv_type == \"valid\":\n",
    "        pad_h = pad_w = 0\n",
    "        out_h = i_h - k_h + 1\n",
    "        out_w = i_w - k_w + 1\n",
    "    else:\n",
    "        raise ValueError(\"conv_type must be 'full', 'same', or 'valid'\")\n",
    "\n",
    "    # Pad the image, put it in the center with 0 outside\n",
    "    padded_img = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant', constant_values=0)\n",
    "\n",
    "    # Perform convolution\n",
    "    result = np.zeros((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            img_region = padded_img[i : i + k_h, j : j + k_w]\n",
    "            result[i, j] = np.sum(img_region * kernel)\n",
    "            \n",
    "    pass\n",
    "    # =========================================================================================================\n",
    "    return result\n",
    "\n",
    "def my_conv3d(kernel, img, conv_type=\"full\"):\n",
    "    \"\"\"\n",
    "    Implement 3D image convolution\n",
    "    :param kernel: float/int array, shape: (ci, h, w, co)\n",
    "    :param img: float/int array, shape: (height, width, channel)\n",
    "    :param conv_type: str, convolution padding choices, should in ['full', 'same', 'valid']\n",
    "    :return conv results, numpy array\n",
    "    \"\"\"\n",
    "    assert len(kernel.shape) == 4 and len(img.shape) == 3, \"The dimensions of kernel and img should be 3.\"\n",
    "    ci, k_h, k_w, co = kernel.shape\n",
    "    i_h, i_w, i_c = img.shape\n",
    "\n",
    "    result = np.zeros((i_h, i_w, i_c))\n",
    "    for c_i in range(ci):\n",
    "      for c_o in range(co):\n",
    "        result[:, :, c_o] = my_conv2d(kernel[c_i, :, :, c_o], img[:, :, c_o], conv_type=conv_type)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def generate_gaussian_kernel(size, sigma = 1):\n",
    "    '''This function generates a 2D Gaussian kernel. And it output a 2D numpy array whose values are the Gaussian values between 0 and 1.'''\n",
    "    # generate an axis\n",
    "    x = np.linspace(-size // 2, size // 2, size)\n",
    "    gauss_x = np.exp(-x ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "    # generate a 2D gaussian kernel and normalize\n",
    "    gauss_2d = np.outer(gauss_x, gauss_x)\n",
    "    gauss_2d /= gauss_2d.sum()\n",
    "    return gauss_2d\n",
    "\n",
    "def gaussian_filter(img, kernel_size, sigma=1):\n",
    "\t\t\"\"\"\n",
    "\t\tImplement Gaussian filter\n",
    "\t\t:param img: float/int array, shape: (height, width)\n",
    "\t\t:param kernel_size: int, the size of the kernel, should be odd number\n",
    "\t\t:param sigma: float, standard deviation of the Gaussian distribution\n",
    "\t\t:return filtered image, numpy array\n",
    "\t\t\"\"\"\n",
    "\t\tassert kernel_size % 2 == 1, \"kernel size should be odd number\"\n",
    "\t\t\n",
    "\t\t# Generate Gaussian kernel\n",
    "\t\tkernel = generate_gaussian_kernel(kernel_size, sigma=sigma)\n",
    "\n",
    "\t\t# Perform convolution using my_conv2d function\n",
    "\t\tfiltered_img = my_conv2d(kernel, img, conv_type=\"same\")\n",
    "\t\t\n",
    "\t\treturn filtered_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_pyramid(image, num_octaves, scales_per_octave, sigma0=1.6):\n",
    "    \"\"\"\n",
    "    Create a Gaussian pyramid with multiple octaves and scales.\n",
    "    \n",
    "    Args:\n",
    "        image: Input grayscale image\n",
    "        num_octaves: Number of octaves\n",
    "        scales_per_octave: Number of scales per octave\n",
    "        sigma0: Initial sigma\n",
    "        \n",
    "    Returns:\n",
    "        gaussian_pyramid: List of octaves, each containing Gaussian blurred images\n",
    "    \"\"\"\n",
    "    # # change to [0, 1] range\n",
    "    # if image.max() > 1:\n",
    "    #     image = image / 255.0\n",
    "\n",
    "    k = 2 ** (1.0 / scales_per_octave)\n",
    "    gaussian_pyramid = []\n",
    "    \n",
    "    # For each octave\n",
    "    for octave in range(num_octaves):\n",
    "        octave_images = []\n",
    "        \n",
    "        # Downsample image for this octave\n",
    "        if octave == 0:\n",
    "            octave_base = image.copy()\n",
    "        else:\n",
    "            octave_base = cv2.resize(gaussian_pyramid[-1][0], \n",
    "                                     (gaussian_pyramid[-1][0].shape[1] // 2, \n",
    "                                     gaussian_pyramid[-1][0].shape[0] // 2))\n",
    "        \n",
    "        # Create scales for this octave\n",
    "        for scale in range(scales_per_octave + 3):\n",
    "            sigma = sigma0 * (k ** scale)\n",
    "            kernel_size = int(2 * np.ceil(2 * sigma) + 1)\n",
    "            blurred = gaussian_filter(octave_base, kernel_size, sigma=sigma)\n",
    "            octave_images.append(blurred)\n",
    "            \n",
    "        gaussian_pyramid.append(octave_images)\n",
    "        \n",
    "    \n",
    "    # show the gaussian pyramid, one plot for each octave, total plot number is num_octaves\n",
    "    for i in range(num_octaves):\n",
    "        plt.figure(figsize=(100, 15))\n",
    "        for j in range(scales_per_octave + 3):\n",
    "            plt.subplot(1, scales_per_octave + 3, j + 1)\n",
    "            plt.imshow(gaussian_pyramid[i][j], cmap='gray')\n",
    "            # plt.axis('off')\n",
    "        print(\"Octave: \", i, \" Scale: \", scales_per_octave + 3, \" Shape: \", gaussian_pyramid[i][j].shape)\n",
    "        sigma_box = [sigma0 * (k ** scale) for scale in range(scales_per_octave + 2)]\n",
    "        print(\"Sigma: \", sigma_box)\n",
    "        #print(\"Gaussian kernel: size\", kernel_size, \"\\n\", generate_gaussian_kernel(kernel_size, sigma=sigma_box[0] * 5))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    return gaussian_pyramid\n",
    "\n",
    "def create_dog_pyramid(gaussian_pyramid):\n",
    "    \"\"\"\n",
    "    Create Difference-of-Gaussian (DoG) pyramid from Gaussian pyramid.\n",
    "    \n",
    "    Args:\n",
    "        gaussian_pyramid: Gaussian pyramid (list of octaves)\n",
    "        \n",
    "    Returns:\n",
    "        dog_pyramid: List of DoG octaves containing difference images\n",
    "    \"\"\"\n",
    "    dog_pyramid = []\n",
    "    initial_shape = gaussian_pyramid[0][0].shape\n",
    "    print(\"Initial Shape: \", initial_shape)\n",
    "    print(\"Initial Image: \", gaussian_pyramid[0][0])\n",
    "    print(\"Second Image: \", gaussian_pyramid[0][1])\n",
    "    \n",
    "    for octave_images in gaussian_pyramid:\n",
    "        # Verify valid input structure\n",
    "        if len(octave_images) < 2:\n",
    "            raise ValueError(\"Octave must contain at least 2 Gaussian images\")\n",
    "            \n",
    "        dog_images = []\n",
    "        # Generate (n-1) DoG images per octave and remember to resize the image to the same size\n",
    "        for i in range(1, len(octave_images)):\n",
    "            dog = octave_images[i] - octave_images[i-1]\n",
    "            if i == 1:\n",
    "              print(\"max: \", np.max(dog), \"shape: \", dog.shape, \"DoG: \", dog)\n",
    "            #dog = cv2.resize(dog, (initial_shape[1], initial_shape[0]))\n",
    "            # change to [0, 1] range\n",
    "            dog = (dog - np.min(dog)) / (np.max(dog) - np.min(dog)) \n",
    "            if i == 1:\n",
    "              print(\"max: \", np.max(dog), \"shape: \", dog.shape, \"DoG: \", dog)\n",
    "            dog_images.append(dog)\n",
    "        dog_pyramid.append(dog_images)\n",
    "    \n",
    "    # show the DoG pyramid, one plot for each octave, total plot number is num_octaves\n",
    "    #'''\n",
    "    for i in range(len(dog_pyramid)):\n",
    "        plt.figure(figsize=(100, 15))\n",
    "        for j in range(len(dog_pyramid[i])):\n",
    "            plt.subplot(1, len(dog_pyramid[i]), j + 1)\n",
    "            plt.imshow(dog_pyramid[i][j], cmap='gray')\n",
    "\t\t\t\t\t\t# plt.axis('off')\n",
    "        print(\"Octave: \", i, \" Scale: \", len(dog_pyramid[i]), \" Shape: \", dog_pyramid[i][j].shape)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    #'''\n",
    "\n",
    "    return dog_pyramid\n",
    "\n",
    "\n",
    "last_detected_number = 500\n",
    "def detect_keypoints2(dog_pyramid, contrast_threshold=0.5, edge_threshold=10):\n",
    "    \"\"\"\n",
    "    Detect keypoints in the DoG pyramid with sub-pixel refinement.\n",
    "    \n",
    "    Args:\n",
    "        dog_pyramid: DoG pyramid from create_dog_pyramid()\n",
    "        contrast_threshold: Minimum contrast (normalized to [0,1])\n",
    "        edge_threshold: Eigenvalue ratio threshold (usually 10)\n",
    "        \n",
    "    Returns:\n",
    "        keypoints: List of keypoints as (octave, scale, y, x)\n",
    "    \"\"\"\n",
    "    keypoints = []\n",
    "    #contrast_threshold = contrast_threshold * 255  # Scale to pixel values\n",
    "    \n",
    "    total_keypoints_num = 0\n",
    "    for octave_idx, dog_octave in enumerate(dog_pyramid):\n",
    "        # Require at least 3 DoG images for scale-space extremum\n",
    "        if len(dog_octave) < 3:\n",
    "            continue\n",
    "        \n",
    "        octave_num = 0\n",
    "        for scale_idx in range(1, len(dog_octave)-1):\n",
    "            prev_dog = dog_octave[scale_idx-1]\n",
    "            curr_dog = dog_octave[scale_idx]\n",
    "            next_dog = dog_octave[scale_idx+1]\n",
    "            \n",
    "            # Iterate through interior pixels (excluding 1-pixel border)\n",
    "            height, width = curr_dog.shape\n",
    "            for i in range(1, height-1):\n",
    "                for j in range(1, width-1):\n",
    "                    # Current pixel value\n",
    "                    val = curr_dog[i, j]\n",
    "\n",
    "                    \n",
    "                    # 3D neighborhood check (3x3x3 cube)\n",
    "                    neighborhood = np.concatenate([\n",
    "                        prev_dog[i-1:i+2, j-1:j+2].flatten(),\n",
    "                        curr_dog[i-1:i+2, j-1:j+2].flatten(),\n",
    "                        next_dog[i-1:i+2, j-1:j+2].flatten()\n",
    "                    ])  \n",
    "                    neighborhood = np.delete(neighborhood, 13)  # Remove the center element\n",
    "                    is_max = val > np.max(neighborhood)\n",
    "                    is_min = val < np.min(neighborhood)\n",
    "\n",
    "                    if not (is_max or is_min):\n",
    "                        continue\n",
    "\n",
    "                    # Edge response check using Hessian matrix\n",
    "                    # Second derivatives (central differences)\n",
    "                    dx = (curr_dog[i, j+1] - curr_dog[i, j-1]) / 2.0\n",
    "                    dy = (curr_dog[i+1, j] - curr_dog[i-1, j]) / 2.0\n",
    "                    dxx = curr_dog[i, j+1] + curr_dog[i, j-1] - 2*val\n",
    "                    dyy = curr_dog[i+1, j] + curr_dog[i-1, j] - 2*val\n",
    "                    dxy = (curr_dog[i+1, j+1] - curr_dog[i+1, j-1] - \n",
    "                          curr_dog[i-1, j+1] + curr_dog[i-1, j-1]) / 4.0\n",
    "                    \n",
    "                     # Solve for offset\n",
    "                    gradient = np.array([dx, dy])\n",
    "                    hessian = np.array([[dxx, dxy], [dxy, dyy]])\n",
    "                    \n",
    "                    try:\n",
    "                        offset = -np.linalg.lstsq(hessian, gradient, rcond=None)[0]\n",
    "                    except np.linalg.LinAlgError:\n",
    "                        continue\n",
    "                        \n",
    "                    if np.abs(offset[0]) > 0.5 or np.abs(offset[1]) > 0.5:\n",
    "                        continue  # Reject unstable refinements\n",
    "\n",
    "                    # 3. Contrast thresholding AFTER refinement\n",
    "                    contrast = curr_dog[i, j] + 0.5 * np.dot(gradient, offset)\n",
    "                    if np.abs(contrast) < contrast_threshold:\n",
    "                        continue\n",
    "\n",
    "                    # 4. Edge rejection\n",
    "                    tr = dxx + dyy\n",
    "                    det = dxx * dyy - dxy**2\n",
    "                    if det <= 0 or tr**2 * edge_threshold >= (edge_threshold + 1)**2 * det:\n",
    "                        continue\n",
    "\n",
    "                    # Store refined coordinates\n",
    "                    keypoints.append((\n",
    "                        octave_idx,\n",
    "                        scale_idx,\n",
    "                        i + offset[1],  # y-coordinate\n",
    "                        j + offset[0]   # x-coordinate\n",
    "                    ))\n",
    "                    octave_num = octave_num + 1\n",
    "                    \n",
    "        print(\"find keypoints in octave: \", octave_idx, \" keypoints number: \", octave_num)\n",
    "        total_keypoints_num = total_keypoints_num + octave_num\n",
    "    print(\"total keypoints number: \", total_keypoints_num)\n",
    "                \n",
    "    if len(keypoints) < 100:\n",
    "        print(\"Warning: Very few keypoints detected, less than 100\")\n",
    "        print(\"redo the keypoints detection...\", \" last contrast_threshold: \", contrast_threshold)\n",
    "        keypoints = detect_keypoints2(dog_pyramid, contrast_threshold=0.8* contrast_threshold, \n",
    "                                      edge_threshold=10)\n",
    "        \n",
    "    if len(keypoints) > 4000 and contrast_threshold < 0.9:\n",
    "        print(\"Warning: Too many keypoints detected, more than 4000\")\n",
    "        print(\"redo the keypoints detection...\", \"last contrast_threshold: \", contrast_threshold)\n",
    "        keypoints = detect_keypoints2(dog_pyramid, contrast_threshold=1.05 * contrast_threshold, edge_threshold=10)\n",
    "        \n",
    "    last_detected_number = len(keypoints)\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def compute_orientations2(gaussian_pyramid, keypoints, num_bins=36, sigma0=1.4):\n",
    "    \"\"\"\n",
    "    Compute dominant orientations for each keypoint.\n",
    "    \n",
    "    Args:\n",
    "        gaussian_pyramid: Gaussian pyramid\n",
    "        keypoints: List of keypoints (octave, scale, y, x)\n",
    "        num_bins: Number of orientation bins\n",
    "        \n",
    "    Returns:\n",
    "        oriented_keypoints: List of keypoints with orientation (octave, scale, y, x, orientation)\n",
    "    \"\"\"\n",
    "    oriented_keypoints = []\n",
    "    \n",
    "    for keypoint in keypoints:\n",
    "        octave_idx, scale_idx, refine_y, refine_x = keypoint\n",
    "        img = gaussian_pyramid[octave_idx][scale_idx]\n",
    "        scales_per_octave = len(gaussian_pyramid[0]) - 2\n",
    "        x = int(refine_x)\n",
    "        y = int(refine_y)\n",
    "        \n",
    "        # Create histogram of orientations\n",
    "        histogram = np.zeros(num_bins)\n",
    "        # Gaussian weighting sigma0=1.6\n",
    "        k = 2 ** (1.0 / scales_per_octave)\n",
    "        sigma = sigma0 * (k ** scale_idx)\n",
    "        radius = int(3 * sigma)\n",
    "        \n",
    "        for i in range(-radius, radius + 1):\n",
    "            yi = y + i\n",
    "            if yi <= 0 or yi >= img.shape[0] - 1:\n",
    "                continue\n",
    "                \n",
    "            for j in range(-radius, radius + 1):\n",
    "                xi = x + j\n",
    "                if xi <= 0 or xi >= img.shape[1] - 1:\n",
    "                    continue\n",
    "                \n",
    "                # Compute gradient\n",
    "                dx = img[yi, xi+1] - img[yi, xi-1]\n",
    "                dy = img[yi+1, xi] - img[yi-1, xi]\n",
    "                \n",
    "                # Compute magnitude and orientation\n",
    "                magnitude = np.sqrt(dx**2 + dy**2)\n",
    "                orientation = np.arctan2(dy, dx) % (2 * np.pi)\n",
    "                \n",
    "                # Weight by magnitude and distance from center\n",
    "                weight = magnitude * np.exp(-(i**2 + j**2) / (2 * sigma**2))\n",
    "                \n",
    "                # Add to histogram\n",
    "                bin_idx = int(orientation / (2 * np.pi) * num_bins) % num_bins\n",
    "                histogram[bin_idx] += weight\n",
    "        \n",
    "        # Smooth histogram\n",
    "        histogram = np.roll(histogram, 1) + histogram + np.roll(histogram, -1)\n",
    "        histogram = histogram / 3.0\n",
    "        \n",
    "        # Find peaks in histogram\n",
    "        threshold = 0.8 * np.max(histogram)\n",
    "        for bin_idx in range(num_bins):\n",
    "            if (histogram[bin_idx] > histogram[(bin_idx-1) % num_bins] and\n",
    "                histogram[bin_idx] > histogram[(bin_idx+1) % num_bins] and\n",
    "                histogram[bin_idx] >= threshold):\n",
    "                \n",
    "                # Convert bin index to angle (in radians)\n",
    "                angle = bin_idx * 2 * np.pi / num_bins\n",
    "                oriented_keypoints.append((octave_idx, scale_idx, refine_y, refine_x, angle))\n",
    "    \n",
    "    return oriented_keypoints\n",
    "\n",
    "\n",
    "def compute_descriptors2(gaussian_pyramid, oriented_keypoints, descriptor_size=4, num_bins=8, sigma0=1.4):\n",
    "    \"\"\"\n",
    "    Compute SIFT descriptors for keypoints.\n",
    "    \n",
    "    Args:\n",
    "        gaussian_pyramid: Gaussian pyramid\n",
    "        oriented_keypoints: List of keypoints with orientation\n",
    "        descriptor_size: Size of descriptor grid (e.g., 4 means 4x4 grid)\n",
    "        num_bins: Number of orientation bins per grid cell\n",
    "        \n",
    "    Returns:\n",
    "        keypoints: List of keypoint locations (x, y, scale, orientation)\n",
    "        descriptors: List of descriptors\n",
    "    \"\"\"\n",
    "    # This function do these things:\n",
    "    # 1. Compute the window size * size , size is 4 * 3 *sigma as int\n",
    "    # 2. Get the orientation of the keypoint from the oriented_keypoints\n",
    "    # 3. slice the window into 4 * 4 cells, and each cell has 8 bins, so the descriptor is 4 * 4 * 8, each cell size is 3 * sigma as int\n",
    "    # 4. For each cell, calculate the gradient magnitude and orientation, and then weight by magnitude and distance from center, according to the orientation, add to the corresponding bin, here we should add the orientation to histogram bins\n",
    "\t\t# 5. Threshold and normalize for illumination invariance\n",
    "    # 6. Add the bins to the descriptor\n",
    "    # 7. Add the descriptor to the descriptors_list, so is the keypoints_list\n",
    "    keypoints_list = []\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for keypoint in oriented_keypoints:\n",
    "        octave_idx, scale_idx, refine_y, refine_x, angle = keypoint\n",
    "        img = gaussian_pyramid[octave_idx][scale_idx]\n",
    "        scales_per_octave = len(gaussian_pyramid[0]) - 2\n",
    "        x = int(refine_x)\n",
    "        y = int(refine_y)\n",
    "        \n",
    "        scale = 2 ** octave_idx\n",
    "        k = 2 ** (1.0 / scales_per_octave)\n",
    "        sigma = sigma0 * (k  ** scale_idx)\n",
    "        \n",
    "        cos_angle = np.cos(angle)\n",
    "        sin_angle = np.sin(angle)\n",
    "        \n",
    "        cell_size = int(3 * sigma)\n",
    "        half_width = (descriptor_size * cell_size) // 2\n",
    "        \n",
    "        descriptor = np.zeros((descriptor_size, descriptor_size, num_bins))\n",
    "        \n",
    "        for cell_i in range(descriptor_size):\n",
    "            for cell_j in range(descriptor_size):\n",
    "                for i in range(cell_size):\n",
    "                    for j in range(cell_size):\n",
    "                        # compute the rotated coordinates\n",
    "                        u = (cell_i - descriptor_size/2) * cell_size + i\n",
    "                        v = (cell_j - descriptor_size/2) * cell_size + j\n",
    "                        x_rot = u * cos_angle - v * sin_angle\n",
    "                        y_rot = u * sin_angle + v * cos_angle\n",
    "                        sample_x = x + x_rot\n",
    "                        sample_y = y + y_rot\n",
    "                        \n",
    "                        if sample_x < 1 or sample_x >= img.shape[1]-1 or sample_y < 1 or sample_y >= img.shape[0]-1:\n",
    "                            continue\n",
    "                        \n",
    "                        dx = img[int(sample_y), int(sample_x)+1] - img[int(sample_y), int(sample_x)-1]\n",
    "                        dy = img[int(sample_y)+1, int(sample_x)] - img[int(sample_y)-1, int(sample_x)]\n",
    "                        magnitude = np.sqrt(dx**2 + dy**2)\n",
    "                        orientation = (np.arctan2(dy, dx) - angle) % (2 * np.pi)\n",
    "                        \n",
    "                        bin_idx = int(orientation / (2 * np.pi) * num_bins) % num_bins\n",
    "                        weight = magnitude * np.exp(-((i - cell_size//2)**2 + (j - cell_size//2)**2) / (2 * (0.5 * cell_size)**2))\n",
    "                        descriptor[cell_i, cell_j, bin_idx] += weight\n",
    "        \n",
    "        flat_descriptor = descriptor.flatten()\n",
    "        threshold = 0.2 * np.linalg.norm(flat_descriptor)\n",
    "        flat_descriptor = np.minimum(flat_descriptor, threshold)\n",
    "        norm = np.linalg.norm(flat_descriptor)\n",
    "        if norm > 0:\n",
    "            flat_descriptor /= norm\n",
    "        \n",
    "        keypoints_list.append((int(refine_x * scale), int(refine_y * scale), scale, angle))\n",
    "        descriptors_list.append(flat_descriptor)\n",
    "    \n",
    "    return keypoints_list, np.array(descriptors_list)\n",
    "   \t\t\t\t\t\t\t         \n",
    "                \n",
    "def match_descriptors(desc1, desc2, ratio_threshold=0.75):\n",
    "    \"\"\"\n",
    "    Match descriptors using ratio test.\n",
    "    \n",
    "    Args:\n",
    "        desc1: First set of descriptors\n",
    "        desc2: Second set of descriptors\n",
    "        ratio_threshold: Ratio test threshold\n",
    "        \n",
    "    Returns:\n",
    "        matches: List of matches (idx1, idx2)\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    \n",
    "    for i, descriptor in enumerate(desc1):\n",
    "        # Compute distances to all descriptors in desc2\n",
    "        distances = []\n",
    "        for descriptor2 in desc2:\n",
    "            diff = descriptor - descriptor2\n",
    "            distance = np.sqrt(np.sum(diff**2))\n",
    "            distances.append(distance)\n",
    "        \n",
    "        # Find indices of two closest matches\n",
    "        idx = np.argsort(distances)\n",
    "        \n",
    "        # Apply ratio test\n",
    "        if distances[idx[0]] < ratio_threshold * distances[idx[1]]:\n",
    "            matches.append((i, idx[0]))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def ransac_match(keypoints1, keypoints2, matches, num_iterations=1000, inlier_threshold=7):\n",
    "    \"\"\"\n",
    "    Match keypoints between two images using RANSAC.\n",
    "    \n",
    "    Args:\n",
    "        keypoints1: List of keypoints from first image\n",
    "        descriptors1: List of descriptors from first image\n",
    "        keypoints2: List of keypoints from second image\n",
    "        descriptors2: List of descriptors from second image\n",
    "        matches: List of matches (idx1, idx2)\n",
    "        num_iterations: Number of RANSAC iterations        \n",
    "        inlier_threshold: Threshold for inliers    \n",
    "\n",
    "    Returns:    \n",
    "        best_inliers: List of best inlier matches\n",
    "        best_homography: Best homography matrix\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    # Need at least 4 points to compute homography\n",
    "    if len(matches) < 4:\n",
    "        return [], None\n",
    "    \n",
    "    # Extract matched points\n",
    "    src_pts = np.float32([keypoints1[match[0]][:2] for match in matches])\n",
    "    dst_pts = np.float32([keypoints2[match[1]][:2] for match in matches])\n",
    "    \n",
    "    best_inliers = []\n",
    "    best_homography = None\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Randomly select 4 matches\n",
    "        sample_indices = random.sample(range(len(matches)), 4)\n",
    "        \n",
    "        # Extract the points from these matches\n",
    "        src_sample = np.float32([src_pts[i] for i in sample_indices])\n",
    "        dst_sample = np.float32([dst_pts[i] for i in sample_indices])\n",
    "        \n",
    "        try:\n",
    "            # Compute homography using the 4 points\n",
    "            homography, _ = cv2.findHomography(src_sample, dst_sample, 0)\n",
    "            \n",
    "            # Skip if homography couldn't be computed\n",
    "            if homography is None:\n",
    "                continue\n",
    "                \n",
    "            # Count inliers\n",
    "            inliers = []\n",
    "            \n",
    "            for i, (src, dst) in enumerate(zip(src_pts, dst_pts)):\n",
    "                # Apply homography to source point\n",
    "                src_transformed = np.dot(homography, np.array([src[0], src[1], 1]))\n",
    "                if src_transformed[2] == 0:  # Check for division by zero\n",
    "                    continue\n",
    "                    \n",
    "                # Convert to (x, y) coordinates\n",
    "                src_transformed = src_transformed[:2] / src_transformed[2]\n",
    "                \n",
    "                # Calculate distance\n",
    "                dist = np.sqrt(np.sum((src_transformed - dst)**2))\n",
    "                \n",
    "                # Check if it's an inlier\n",
    "                if dist < inlier_threshold:\n",
    "                    inliers.append(matches[i])\n",
    "            \n",
    "            # Update best result if this has more inliers\n",
    "            if len(inliers) > len(best_inliers):\n",
    "                best_inliers = inliers\n",
    "                best_homography = homography\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Skip this iteration if there's an error\n",
    "            continue\n",
    "    \n",
    "    return best_inliers, best_homography\n",
    "        \n",
    "        \n",
    "def visualize_keypoints(image, keypoints):\n",
    "\t\"\"\"\n",
    "\tVisualize keypoints on an image.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\timage: Input image\n",
    "\t\tkeypoints: List of keypoints (x, y, scale, orientation)\n",
    "\t\"\"\"\n",
    "\tplt.figure(figsize=(10, 8))\n",
    "\tplt.imshow(image, cmap='gray')\n",
    "\t\n",
    "\t# Create a color cycle for different keypoints\n",
    "\tcolors = plt.cm.hsv(np.linspace(0, 1, len(keypoints)))\n",
    "\t\n",
    "\tfor i, kp in enumerate(keypoints):\n",
    "\t\tx, y, scale, orientation = kp\n",
    "\t\t\n",
    "\t\tradius = scale * 3\n",
    "\t\t\n",
    "\t\t# Draw keypoint circle with unique color\n",
    "\t\tcircle = plt.Circle((x, y), radius, fill=False, color=colors[i])\n",
    "\t\tplt.gca().add_patch(circle)\n",
    "\t\t\n",
    "\t\t# Draw orientation line with same color\n",
    "\t\tline_x = x + radius * np.cos(orientation)\n",
    "\t\tline_y = y + radius * np.sin(orientation)\n",
    "\t\tplt.plot([x, line_x], [y, line_y], color=colors[i])\n",
    "\t\n",
    "\tplt.axis('off')\n",
    "\tplt.title(\"keypoints num: \" + str(len(keypoints))) # Use f-string formatting\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "def visualize_matches(img1, kp1, img2, kp2, matches):\n",
    "\t\"\"\"\n",
    "\tVisualize matches between two images.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\timg1: First image\n",
    "\t\tkp1: Keypoints in first image\n",
    "\t\timg2: Second image\n",
    "\t\tkp2: Keypoints in second image\n",
    "\t\tmatches: List of matches (idx1, idx2)\n",
    "\t\"\"\"\n",
    "\t# Create a new image with both images side by side\n",
    "\th1, w1 = img1.shape[:2]\n",
    "\th2, w2 = img2.shape[:2]\n",
    "\t\n",
    "\th = max(h1, h2)\n",
    "\tw = w1 + w2\n",
    "\t\n",
    "\tvis = np.zeros((h, w), dtype=np.uint8)\n",
    "\tvis[:h1, :w1] = img1 if len(img1.shape) == 2 else cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
    "\tvis[:h2, w1:w1+w2] = img2 if len(img2.shape) == 2 else cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
    "\t\n",
    "\tplt.figure(figsize=(12, 8))\n",
    "\tplt.imshow(vis, cmap='gray')\n",
    "\t\n",
    "\t# Create color map\n",
    "\tcolors = plt.cm.hsv(np.linspace(0, 1, len(matches)))\n",
    "\t\n",
    "\t# Draw lines between matches\n",
    "\tfor i, match in enumerate(matches):\n",
    "\t\tidx1, idx2 = match\n",
    "\t\tx1, y1 = kp1[idx1][0], kp1[idx1][1]\n",
    "\t\tx2, y2 = kp2[idx2][0] + w1, kp2[idx2][1]\n",
    "\t\t\n",
    "\t\t# Use a different color for each line\n",
    "\t\tplt.plot([x1, x2], [y1, y2], color=colors[i])\n",
    "\t\t\n",
    "\t\t# Draw small circles at each vertex\n",
    "\t\tplt.plot(x1, y1, 'o', color=colors[i], markersize=5)\n",
    "\t\tplt.plot(x2, y2, 'o', color=colors[i], markersize=5)\n",
    "\t\n",
    "\tplt.axis('off')\n",
    "\tplt.title(\"Matches: \" + str(len(matches))) # Use f-string formatting\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "def sift(gray_image, num_octaves=4, scales_per_octave=4, contrast_threshold=0.6, edge_threshold=10):\n",
    "    \"\"\"\n",
    "    SIFT feature detection and description pipeline.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        num_octaves: Number of octaves\n",
    "        scales_per_octave: Number of scales per octave\n",
    "        contrast_threshold: Threshold for low contrast keypoints\n",
    "        edge_threshold: Threshold for edge response\n",
    "        \n",
    "    Returns:\n",
    "        keypoints: List of keypoint locations\n",
    "        descriptors: Array of descriptors\n",
    "    \"\"\"\n",
    "    # If not a grayscale image, convert it\n",
    "    if len(gray_image.shape) > 2:\n",
    "        gray_image = cv2.cvtColor(gray_image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "\t\t# If not a float image, convert it\n",
    "    if gray_image.dtype != np.float32:\n",
    "        gray_image = gray_image.astype(np.float32)\n",
    "    \n",
    "    # Create Gaussian pyramid\n",
    "    gaussian_pyr = create_gaussian_pyramid(gray_image, num_octaves, scales_per_octave)\n",
    "    \n",
    "    # Create DoG pyramid\n",
    "    dog_pyr = create_dog_pyramid(gaussian_pyr)\n",
    "    \n",
    "    # Detect keypoints\n",
    "    keypoints = detect_keypoints2(dog_pyr, contrast_threshold, edge_threshold)\n",
    "    \n",
    "    # Compute orientations\n",
    "    oriented_keypoints = compute_orientations2(gaussian_pyr, keypoints)\n",
    "    \n",
    "    # Compute descriptors\n",
    "    keypoints_list, descriptors = compute_descriptors2(gaussian_pyr, oriented_keypoints)\n",
    "    \n",
    "    return keypoints_list, descriptors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Images\n",
    "\n",
    "Let's load two images to test our SIFT implementation. You can replace these with your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your images\n",
    "# Replace these paths with the path to your own images\n",
    "img1_path = 'roof1.png'  # Replace with your image path\n",
    "img2_path = 'roof2.png'  # Replace with your image path\n",
    "img3_path = 'school_gate.jpeg'  # Replace with your image path\n",
    "\n",
    "img1 = cv2.imread(img1_path)\n",
    "img2 = cv2.imread(img2_path)\n",
    "img3 = cv2.imread(img3_path)\n",
    "\n",
    "# Convert from BGR to RGB for visualization\n",
    "img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "img3_rgb = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the images\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1_rgb)\n",
    "plt.title('Image 1')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2_rgb)\n",
    "plt.title('Image 2')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Change the images to grayscale\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "img3_gray = cv2.cvtColor(img3, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# show the grayscale images\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1_gray, cmap='gray')\n",
    "plt.title('Image 1')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2_gray, cmap='gray')\n",
    "plt.title('Image 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect SIFT Features\n",
    "\n",
    "Now we'll detect SIFT features in both images using our custom implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for SIFT\n",
    "num_octaves = 4\n",
    "scales_per_octave = 4\n",
    "contrast_threshold = 0.7\n",
    "edge_threshold = 10\n",
    "\n",
    "img1_gray = img1_gray.astype(np.float32)\n",
    "img2_gray = img2_gray.astype(np.float32)\n",
    "img3_gray = img3_gray.astype(np.float32)\n",
    "# Detect SIFT features in the first image\n",
    "print(\"Detecting SIFT features in image 1...\")\n",
    "keypoints1, descriptors1 = sift(img1_gray, num_octaves, scales_per_octave, contrast_threshold, edge_threshold)\n",
    "print(f\"Found {len(keypoints1)} keypoints in image 1\")\n",
    "\n",
    "# Detect SIFT features in the second image\n",
    "print(\"\\nDetecting SIFT features in image 2...\")\n",
    "keypoints2, descriptors2 = sift(img2_gray, num_octaves, scales_per_octave, contrast_threshold, edge_threshold)\n",
    "print(f\"Found {len(keypoints2)} keypoints in image 2\")\n",
    "\n",
    "keypoints3, descriptors3 = sift(img3_gray, num_octaves, scales_per_octave, contrast_threshold, edge_threshold)\n",
    "print(f\"Found {len(keypoints3)} keypoints in image 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Detected Keypoints\n",
    "\n",
    "Let's visualize the detected keypoints on both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize keypoints on the first image\n",
    "print(\"Keypoints in image 1: numbers of keypoints: \", len(keypoints1))\n",
    "visualize_keypoints(img1_rgb, keypoints1)\n",
    "\n",
    "# Visualize keypoints on the second image\n",
    "print(\"Keypoints in image 2: numbers of keypoints: \", len(keypoints2))\n",
    "visualize_keypoints(img2_rgb, keypoints2)\n",
    "\n",
    "print(\"Keypoints in image 3: numbers of keypoints: \", len(keypoints3))\n",
    "visualize_keypoints(img3_rgb, keypoints3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Match Keypoints Between Images\n",
    "\n",
    "Now let's match keypoints between the two images using the ratio test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match descriptors between images\n",
    "ratio_threshold = 0.85  # Lowe's ratio test threshold\n",
    "matches = match_descriptors(descriptors1, descriptors2, ratio_threshold)\n",
    "\n",
    "print(f\"Found {len(matches)} matches between the images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Matches\n",
    "\n",
    "Finally, let's visualize the matches between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the matches\n",
    "visualize_matches(img1_gray, keypoints1, img2_gray, keypoints2, matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment: Effect of Different Parameters\n",
    "\n",
    "Let's see how changing the parameters affects the SIFT detection and matching results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different parameters to try\n",
    "contrast_thresholds = [0.55, 0.75, 0.8]\n",
    "edge_thresholds = [5, 10, 15]\n",
    "\n",
    "# A sample experiment with different contrast thresholds\n",
    "for threshold in contrast_thresholds:\n",
    "    print(f\"\\nUsing contrast threshold = {threshold}\")\n",
    "    \n",
    "    # Detect features with this threshold\n",
    "    kp1, desc1 = sift(img1_gray, num_octaves, scales_per_octave, threshold, edge_threshold)\n",
    "    kp2, desc2 = sift(img2_gray, num_octaves, scales_per_octave, threshold, edge_threshold)\n",
    "    \n",
    "    print(f\"Image 1: {len(kp1)} keypoints\")\n",
    "    print(f\"Image 2: {len(kp2)} keypoints\")\n",
    "    \n",
    "    # Match the features\n",
    "    matches = match_descriptors(desc1, desc2, ratio_threshold)\n",
    "    print(f\"Matches: {len(matches)}\")\n",
    "    \n",
    "    # Visualize a few keypoints (just to avoid too many plots)\n",
    "    if len(kp1) > 0 and len(kp2) > 0 and len(matches) > 0:\n",
    "        visualize_matches(img1_rgb, kp1, img2_rgb, kp2, matches[:50] if len(matches) > 50 else matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Try with Different Images\n",
    "\n",
    "You can replace the image paths at the beginning of this notebook to try with different images. Here's a function to make it easier to test with new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sift_on_images(img1_path, img2_path, num_octaves=4, scales_per_octave=4, \n",
    "                        contrast_threshold=0.6, edge_threshold=10, ratio_threshold=0.75):\n",
    "    \"\"\"Test SIFT feature detection and matching on two images\"\"\"\n",
    "    # Load images\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display images\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img1_rgb)\n",
    "    plt.title('Image 1')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.imshow(img2_rgb)\n",
    "    plt.title('Image 2')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detect SIFT features\n",
    "    print(\"Detecting features...\")\n",
    "    img1_rgb = img1_rgb.astype(np.float32)\n",
    "    img2_rgb = img2_rgb.astype(np.float32)\n",
    "    kp1, desc1 = sift(img1_rgb, num_octaves, scales_per_octave, contrast_threshold, edge_threshold)\n",
    "    kp2, desc2 = sift(img2_rgb, num_octaves, scales_per_octave, contrast_threshold, edge_threshold)\n",
    "    \n",
    "    print(f\"Found {len(kp1)} keypoints in image 1\")\n",
    "    print(f\"Found {len(kp2)} keypoints in image 2\")\n",
    "    \n",
    "    # Visualize keypoints\n",
    "    img1_rgb = img1_rgb.astype(np.uint8)\n",
    "    img2_rgb = img2_rgb.astype(np.uint8)\n",
    "    visualize_keypoints(img1_rgb, kp1)\n",
    "    visualize_keypoints(img2_rgb, kp2)\n",
    "    \n",
    "    # Match features\n",
    "    matches = match_descriptors(desc1, desc2, ratio_threshold)\n",
    "    print(f\"Found {len(matches)} matches between the images\")\n",
    "    \n",
    "    # Visualize matches\n",
    "    visualize_matches(img1_rgb, kp1, img2_rgb, kp2, matches)\n",
    "    \n",
    "    return kp1, desc1, kp2, desc2, matches\n",
    "\n",
    "# You can use this function to test with your own images\n",
    "#kp1, desc1, kp2, desc2, matches = test_sift_on_images('books1.jpeg', 'books2.jpeg', num_octaves=4, scales_per_octave=4,\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontrast_threshold=0.6, edge_threshold=10, ratio_threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1, kp2, desc2, matches = test_sift_on_images('building_2.jpg', 'building_2d.jpg', num_octaves=4, scales_per_octave=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontrast_threshold=0.6, edge_threshold=10, ratio_threshold=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ransac\n",
    "img1_rgb = cv2.cvtColor(cv2.imread('building_2.jpg'), cv2.COLOR_BGR2RGB)\n",
    "img2_rgb = cv2.cvtColor(cv2.imread('building_2d.jpg'), cv2.COLOR_BGR2RGB)\n",
    "inliers, homography = ransac_match(kp1, kp2, matches, num_iterations=1000, inlier_threshold=7)\n",
    "print(\"original matches: \", len(matches))\n",
    "print(f\"Found {len(inliers)} inliers\")\n",
    "print(\"Homography matrix:\")\n",
    "print(homography)\n",
    "visualize_matches(img1_rgb, kp1, img2_rgb, kp2, matches)\n",
    "visualize_matches(img1_rgb, kp1, img2_rgb, kp2, inliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1, kp2, desc2, matches = test_sift_on_images('mars1.jpg', 'mars2.jpg', num_octaves=4, scales_per_octave=4,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcontrast_threshold=0.6, edge_threshold=10, ratio_threshold=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rotation and Scale Invariance Test\n",
    "\n",
    "Let's test whether our SIFT implementation is truly invariant to rotation and scaling by applying transformations to one of our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_invariance(img_path):\n",
    "    \"\"\"Test rotation and scale invariance of SIFT\"\"\"\n",
    "    # Load the original image\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a rotated version (45 degrees)\n",
    "    rows, cols = img.shape[:2]\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1)\n",
    "    img_rotated = cv2.warpAffine(img, rotation_matrix, (cols, rows))\n",
    "    img_rotated_rgb = cv2.cvtColor(img_rotated, cv2.COLOR_BGR2RGB)\n",
    "    #save the rotation image\n",
    "    cv2.imwrite(\"building_2r45.jpg\", img_rotated)\n",
    "    \n",
    "    # Create a scaled version (0.75x)\n",
    "    img_scaled = cv2.resize(img, None, fx=0.75, fy=0.75)\n",
    "    img_scaled_rgb = cv2.cvtColor(img_scaled, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the three images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(131)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(img_rotated_rgb)\n",
    "    plt.title('Rotated 45°')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(img_scaled_rgb)\n",
    "    plt.title('Scaled 0.75x')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Test with original vs. rotated\n",
    "    print(\"\\nTesting Original vs. Rotated:\")\n",
    "    # change to float32\n",
    "    img_rgb = img_rgb.astype(np.float32)\n",
    "    img_rotated_rgb = img_rotated_rgb.astype(np.float32)\n",
    "    img_scaled_rgb = img_scaled_rgb.astype(np.float32)\n",
    "    \n",
    "    kp_orig, desc_orig = sift(img_rgb, num_octaves=4, scales_per_octave=4, contrast_threshold=0.6, edge_threshold=10)\n",
    "    kp_rot, desc_rot = sift(img_rotated_rgb, num_octaves=4, scales_per_octave=4, contrast_threshold=0.6, edge_threshold=10)\n",
    "    img_rgb = img_rgb.astype(np.uint8)\n",
    "    img_rotated_rgb = img_rotated_rgb.astype(np.uint8)\n",
    "    visualize_keypoints(img_rotated_rgb, kp_rot)\n",
    "    visualize_keypoints(img_rgb, kp_orig)\n",
    "    matches_rot = match_descriptors(desc_orig, desc_rot, ratio_threshold=0.75)\n",
    "    print(f\"Found {len(matches_rot)} matches between original and rotated images\")\n",
    "    visualize_matches(img_rgb, kp_orig, img_rotated_rgb, kp_rot, matches_rot)\n",
    "    \n",
    "    # Test with original vs. scaled\n",
    "    print(\"\\nTesting Original vs. Scaled:\")\n",
    "    kp_scaled, desc_scaled = sift(img_scaled_rgb, num_octaves=4, scales_per_octave=4, contrast_threshold=0.6, edge_threshold=10)\n",
    "    matches_scaled = match_descriptors(desc_orig, desc_scaled, ratio_threshold=0.75)\n",
    "    img_scaled_rgb = img_scaled_rgb.astype(np.uint8)\n",
    "    visualize_keypoints(img_scaled_rgb, kp_scaled)\n",
    "    print(f\"Found {len(matches_scaled)} matches between original and scaled images\")\n",
    "    visualize_matches(img_rgb, kp_orig, img_scaled_rgb, kp_scaled, matches_scaled)\n",
    "\n",
    "# You can use this function to test invariance with your own image\n",
    "test_invariance('building_2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Comparison with OpenCV SIFT\n",
    "\n",
    "Let's compare our SIFT implementation with OpenCV's built-in SIFT in terms of keypoint detection and matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_opencv_sift(img1_path, img2_path):\n",
    "    \"\"\"Compare our SIFT implementation with OpenCV's built-in SIFT\"\"\"\n",
    "    # Load images\n",
    "    img1 = cv2.imread(img1_path)\n",
    "    img2 = cv2.imread(img2_path)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Our SIFT implementation\n",
    "    print(\"\\nUsing our SIFT implementation:\")\n",
    "    \n",
    "    # Measure time\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    kp1_our, desc1_our = sift(img1_rgb)\n",
    "    kp2_our, desc2_our = sift(img2_rgb)\n",
    "    matches_our = match_descriptors(desc1_our, desc2_our)\n",
    "    \n",
    "    our_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Found {len(kp1_our)} keypoints in image 1\")\n",
    "    print(f\"Found {len(kp2_our)} keypoints in image 2\")\n",
    "    print(f\"Found {len(matches_our)} matches between images\")\n",
    "    print(f\"Time taken: {our_time:.2f} seconds\")\n",
    "    \n",
    "    # OpenCV SIFT implementation\n",
    "    print(\"\\nUsing OpenCV SIFT implementation:\")\n",
    "    \n",
    "    # Create OpenCV SIFT detector\n",
    "    sift_cv = cv2.SIFT_create()\n",
    "    \n",
    "    # Measure time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Detect and compute with OpenCV SIFT\n",
    "    kp1_cv, desc1_cv = sift_cv.detectAndCompute(img1, None)\n",
    "    kp2_cv, desc2_cv = sift_cv.detectAndCompute(img2, None)\n",
    "    \n",
    "    # Match with OpenCV BFMatcher\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches_cv = bf.knnMatch(desc1_cv, desc2_cv, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches_cv:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    cv_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Found {len(kp1_cv)} keypoints in image 1\")\n",
    "    print(f\"Found {len(kp2_cv)} keypoints in image 2\")\n",
    "    print(f\"Found {len(good_matches)} matches between images\")\n",
    "    print(f\"Time taken: {cv_time:.2f} seconds\")\n",
    "    \n",
    "    # Display OpenCV SIFT matches\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title('OpenCV SIFT Matches')\n",
    "    img_matches = cv2.drawMatches(img1, kp1_cv, img2, kp2_cv, good_matches, None, flags=2)\n",
    "    plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display our SIFT matches\n",
    "    print(\"\\nOur SIFT Implementation Matches:\")\n",
    "    visualize_matches(img1_rgb, kp1_our, img2_rgb, kp2_our, matches_our)\n",
    "\n",
    "# You can use this function to compare implementations\n",
    "compare_with_opencv_sift('books1.jpeg', 'books2.jpeg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
